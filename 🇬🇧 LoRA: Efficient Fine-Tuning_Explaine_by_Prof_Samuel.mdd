<br>

# ðŸ‡ºðŸ‡¸ English Version

> **Based on the explanation by Professor [Samuel Fernando](https://www.linkedin.com/in/samuelfernando2030/)**
> *Senior AI/ML Engineer | Researcher | MSc Quantum Computing | LinkedIn Top Voice*

<br><br>


##  What is LoRA?

LoRA (Low-Rank Adaptation) is a foundational technique for **efficient fine-tuning** of large models. As pointed out by **Professor Samuel Fernando**, many crucial advances were happening long before the GenAI hype.

Published in 2021 for GPT-3, LoRA made it possible to adapt huge models without touching billions of parameters.

<br><br>


##  How it works

Traditional fine-tuning updates the entire weight matrix **W**.
LoRA changes this:

<br><br>


## âœ” The base matrix W is frozen

### âœ” Only a low-rank update Î”W is learned:

```text
W' = W + Î”W
Î”W = B * A
```


<br>

Where **A** and **B** are low-rank matrices:

<br>

* A: (r Ã— k)
* B: (d Ã— r)
* with r â‰ª d and k.

<br><br>

##  Numerical Example

A 1000 Ã— 1000 matrix has **1,000,000 parameters**.

<br>

### With LoRA you only update:

* A: r Ã— 1000
* B: 1000 Ã— r

If r = 8 â†’ **only 16,000 parameters**.
A massive reduction with minimal performance loss.

<br><br>



## Core insight

The meaningful update after massive pretraining **lives in a very low-dimensional subspace**.
Thatâ€™s why LoRA is so efficient.

<br><br>

## Impact on GenAI

The paper â†’ library â†’ industry standard â†’ product cycle became incredibly short.
LoRA is now a cornerstone of modern PEFT techniques.

<br><br>

## ðŸŽ“ Academic relevance

As Professor [**Samuel Fernando**]() emphasizes, academic research remains indispensable â€” many breakthroughs predate the GenAI popularization.

