<br>
 
 \[[üáßüá∑ Portugu√™s](README.pt_BR.md)\] \[**[üá∫üá∏ English](README.md)**\]

<br><br>


# <p align="center"> Fine-tuning Ministral-3 with [Unsloth]() üî• Guia Completo
### <p align="center"> Treino acelerado, otimizado e barato usando Unsloth + Ministral-3.

<br><br>



Este reposit√≥rio fornece um ambiente completo, r√°pido e moderno para fine-tuning, infer√™ncia, curadoria de dados e exporta√ß√£o de modelos Ministral-3, Llama, Qwen, Gemma, DeepSeek e variantes, utilizando o ecossistema Unsloth.

Inclui notebooks prontos, scripts de treino, exemplos de datasets, Docker, exporta√ß√£o para GGUF/Ollama/vLLM e suporte a Reinforcement Learning (GRPO, DPO, ORPO, KTO).



<br><br><br>



### <p align="center"> [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](#license) [![Sponsor Mindful AI Assistants](https://img.shields.io/badge/Sponsor-Mindful%20AI%20%20Assistants-brightgreen?logo=GitHub)](https://github.com/sponsors/Mindful-AI-Assistants) [![Python](https://img.shields.io/badge/Python-‚â§3.13-blue)](#installation)


<br><br><br>



> [!NOTE]
>
> [-]()  Ambiente completo de fine-tuning para LLMs usando Unsloth, incluindo <br>
> [-]()  Ministral 3, Qwen, Llama, DeepSeek, Gemma, RL, Vision, exporta√ß√£o GGUF e deployment em produ√ß√£o. <br>
> [-]()  Fonte: [Unsloth ‚Äì Instala√ß√£o & Atualiza√ß√£o](https://docs.unsloth.ai/get-started/install-and-update)  <br> <br>

<br><br>


## [Inclui]():

<br>

[-]() Jupyter notebooks

[-]() Scripts de treinamento, avalia√ß√£o e infer√™ncia

[-]() Exemplos de datasets

[-]() üê≥ Imagens Docker

[-]() Suporte completo ao Unsloth

[-]() üî• Quickstart do Ministral 3




> [!TIP]
>
> * **Fine-tuning & Reinforcement Learning** para LLMs modernos com **at√© 2√ó mais velocidade de treino** e **70% menos uso de VRAM**. <br>
> * **Ambiente completo de fine-tuning** para LLMs usando **Unsloth**, incluindo <br>
> * **Ministral 3**, **Qwen**, **Llama**, **DeepSeek**, **Gemma**, RL, Vision, exporta√ß√£o GGUF e deployment em produ√ß√£o. <br>
>  <br>
>  

<br><br><br>



## Ind√≠ce

- [Introdu√ß√£o](#introdu√ß√£o)
- [Features](#features)
- [Instala√ß√£o](#instala√ß√£o)
  - [Pip](#pip)
  - [Conda](#conda)
  - [Docker](#docker)
  - [Windows](#windows)
  - [Google Colab](#google-colab)
- [Guia de Fine-tuning](#guia-de-fine-tuning)
  - [Escolha de Modelo](#escolha-de-modelo)
  - [Estrutura de Dataset](#estrutura-de-dataset)
  - [Hiperpar√¢metros LoRA](#hiperpar√¢metros-lora)
  - [Vision Fine-tuning](#vision-fine-tuning)
- [Ministral-3 Quickstart](#ministral-3-quickstart)
- [Notebooks](#notebooks)
- [Scripts](#scripts)
- [Estrutura do Reposit√≥rio](#estrutura-do-reposit√≥rio)
- [Deployment & Export](#deployment--export)
  - [Ollama](#ollama)
  - [vLLM](#vllm)
  - [GGUF](#gguf)
- [Troubleshooting](#troubleshooting)
- [Comunidade & Suporte](#comunidade--suporte)
- [License](#license)


<br><br>


## [Introdu√ß√£o]()


Este reposit√≥rio consolida um ambiente robusto e padronizado para:

* **Fine-tuning eficiente com LoRA/QLoRA**
* **Aprendizado por Refor√ßo (GRPO, DPO, ORPO, KTO)**
* **Treinamento em Vis√£o (VLMs)**
* **Exporta√ß√£o para GGUF e deployment em CPU/GPU**
* **Infer√™ncia otimizada com Unsloth e vLLM**
* **Ambiente de desenvolvimento reprodut√≠vel (Docker + Conda)**


<br><br>

## [Suporte completo para:]()

* **Ministral-3 (todos os tamanhos)**
* Llama 3.x
* Qwen 2.5 / 3 / VL
* Gemma 3
* DeepSeek V3 / R1
* Phi 3
* Vision LLMs


<br><br>

##  [Features]()


* ‚ö° *At√© 2√ó mais r√°pido* que frameworks tradicionais
*  *70% menos VRAM* com QLoRA
*  Suporte completo para **Fine-tuning em Vis√£o**
*  Suporte para **RL (GRPO / DPO / ORPO / KTO)**
*  *Contexto ultra-longo* (at√© 500K tokens)
*  Exporta√ß√£o para **GGUF**, **Ollama**, **vLLM**, **safetensors**
*  Docker + scripts padronizados
*  Notebooks para Colab / uso local
*  CPU, CUDA 11.8 / 12.1, AMD ROCm



<br><br>


## [Instala√ß√£o]()

### ‚ö° [Pip Install]()

<br>

```bash
pip install unsloth
```Ô∏è
<br><br>

### üêç [Conda Install]()

<br>

```bash
conda create --name unsloth_env python=3.11 -y
conda activate unsloth_env
pip install unsloth
```

<br><br>

### üê≥ [Docker]()

<br>

```bash
docker pull unslothai/unsloth:latest
```

<br><br>

### [Suporte Windows]()

<br>

‚úî Via WSL2 (recomendado)

‚úî CUDA 12.1

‚úî Apenas CPU


<br><br>


##  [Google Colab]()

<br>

Notebooks oficiais:
https://docs.unsloth.ai/get-started/beginner-start-here


<br>

[Instala√ß√£o r√°pida:]()

<br>

```bash
!pip install unsloth
```

<br><br>

##  [Guia de Fine-tuning]()


###  [Qual modelo escolher ?]()

<br>

| [Tarefa]()                | Modelo recomendado |
| --------------------- | ------------------ |
| [Chat / Agentes]()        | Instruct           |
| [Racioc√≠nio]()            | Base               |
| [Dataset pequeno (<3k)]() | Instruct           |
| [Dataset grande (>20k)]() | Base               |



<br><br>


## [Estrutura do Dataset]()

### [Formato padr√£o (JSONL):]()

<br>

```json
{
  "messages": [
    {"role": "user", "content": "Ol√°"},
    {"role": "assistant", "content": "Oi! Como posso ajudar?"}
  ]
}
```

<br><br>

## [Hiperpar√¢metros LoRA]()

### [Recomenda√ß√£o inicial:]()

<br>

```√¨ni
r = 16
alpha = 32
dropout = 0.05
target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
```

<br><br>


## [Vision Fine-tuning]()

### [Suporte para:]()

<br>

* Ministral-3 Vision

* Qwen-VL

* Gemma Vision


<br><br>


## üî• [Ministral-3 Quickstart]()

###  [Exemplos de modelos suportados]()

<br>

* Ministral-3 Small

* Ministral-3 Medium

* Ministral-3 14B (cabe no Colab Free com QLoRA)


<br><br>

## [Notebook oficial do repo]()


```bash
notebooks/ministral3_finetune.ipynb
```

<br><br>

## C√≥digo de treino (exemplo)()

<br>


```python
from unsloth import FastLanguageModel

model = FastLanguageModel.from_pretrained(
    "unsloth/ministral-3-14b",
    max_seq_length=4096,
)

model = FastLanguageModel.get_peft_model(model)
```


<br><br>

## [Notebooks]()

<br>

| [Notebook]()                | [Descri√ß√£o]()                     | [Link]()                                   |
| ----------------------- | ----------------------------- | -------------------------------------- |
| [Beginner Start Here]()     | Introdu√ß√£o e primeiros passos | notebooks/00_beginner_start_here.ipynb |
| [Ministral-3 Fine-tuning]() | Treino completo               | notebooks/ministral3_finetune.ipynb    |
| [GRPO RL]()                 | Racioc√≠nio com RL             | notebooks/rl/grpo_ministral3.ipynb     |
| [DPO Qwen]()                | RL DPO                        | notebooks/rl/dpo_qwen3.ipynb           |


<br><br>


## [Oficial Unsloth Notebooks]() 

<br>

| Notebook                             | Descri√ß√£o                                                | Link                                                                                                                                                                                                             |
| ------------------------------------ | -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Beginner Start Here**              | Introdu√ß√£o, instala√ß√£o, primeiros passos                 | [https://docs.unsloth.ai/get-started/beginner-start-here](https://docs.unsloth.ai/get-started/beginner-start-here)                                                                                               |
| **Fine-tuning Llama-3 (QLoRA)**      | Fine-tuning padr√£o com LoRA/QLoRA                        | [https://colab.research.google.com/github/unslothai/notebooks/blob/main/examples/fine_tune_llama3.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/examples/fine_tune_llama3.ipynb) |
| **Ministral 3 Fine-tuning**          | Fine-tuning completo dos modelos Ministral 3 (com vis√£o) | [https://docs.unsloth.ai/ministral-3-how-to-run-and-fine-tune](https://docs.unsloth.ai/ministral-3-how-to-run-and-fine-tune)                                                                                     |
| **Vision Fine-tuning**               | Fine-tuning de modelos de vis√£o                          | [https://docs.unsloth.ai/vision-fine-tuning](https://docs.unsloth.ai/vision-fine-tuning)                                                                                                                         |
| **DeepSeek Fine-tuning**             | Treinar e rodar DeepSeek com Unsloth                     | [https://docs.unsloth.ai/deepseek-how-to-run-and-fine-tune](https://docs.unsloth.ai/deepseek-how-to-run-and-fine-tune)                                                                                           |
| **Gemma 3 Fine-tuning**              | Tutorial oficial para Gemma 3                            | [https://docs.unsloth.ai/gemma-3-how-to-run-and-fine-tune](https://docs.unsloth.ai/gemma-3-how-to-run-and-fine-tune)                                                                                             |
| **Qwen3 Fine-tuning**                | Treinar Qwen3 localmente com Unsloth                     | [https://docs.unsloth.ai/qwen3-how-to-run-and-fine-tune](https://docs.unsloth.ai/qwen3-how-to-run-and-fine-tune)                                                                                                 |
| **Qwen3-VL Vision**                  | Fine-tuning multimodal                                   | [https://docs.unsloth.ai/qwen3-vl-how-to-run-and-fine-tune](https://docs.unsloth.ai/qwen3-vl-how-to-run-and-fine-tune)                                                                                           |
| **gpt-oss Training**                 | Fine-tuning dos modelos gpt-oss                          | [https://docs.unsloth.ai/gpt-oss-how-to-run-and-fine-tune](https://docs.unsloth.ai/gpt-oss-how-to-run-and-fine-tune)                                                                                             |
| **Reinforcement Learning (GRPO)**    | Treinar modelos de racioc√≠nio                            | [https://docs.unsloth.ai/Tutorial-train-your-own-reasoning-model-with-grpo](https://docs.unsloth.ai/Tutorial-train-your-own-reasoning-model-with-grpo)                                                           |
| **FP8 Reinforcement Learning**       | RL otimizado com FP8                                     | [https://docs.unsloth.ai/fp8-reinforcement-learning](https://docs.unsloth.ai/fp8-reinforcement-learning)                                                                                                         |
| **Ultra Long Context (500K tokens)** | Fine-tuning com contextos gigantes                       | [https://docs.unsloth.ai/500k-context-length-fine-tuning](https://docs.unsloth.ai/500k-context-length-fine-tuning)                                                                                               |


<br><br>

## [Scripts]()

<br>

```bash
scripts/train.py
scripts/eval.py
scripts/infer.py
```


<br><br>


## [Estrutura do Reposit√≥rio]()

<br>


```javascript
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 00_beginner_start_here.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ ministral3_finetune.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ rl/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îú‚îÄ‚îÄ infer.py
‚îÇ   ‚îî‚îÄ‚îÄ eval.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ samples/
‚îî‚îÄ‚îÄ docker/
    ‚îî‚îÄ‚îÄ Dockerfile
```


<br><br>

## [Deployment & Export()

<br>

```bash
ollama create mymodel -f ollama_modelfile
```

<br><br>


## [vLLM]()


<br>

```bash
python -m vllm.entrypoints.api_server --model ./output
```


<br><br>


## [Exportar para GGUF]()

<br>

```bash
unsloth convert --to-gguf output/
```

<br><br>


## ‚ùó [Resolu√ß√£o de Problemas()

<br>

* Incompatibilidade de CUDA

* OOM ‚Üí reduza o rank do LoRA

* Incompatibilidade do Tokenizer ‚Üí use matching safetensors


<br><br>


## [Comunidade & Suporte]()

<br>

* [Reddit](r/unsloth)

* [Docs oficiais](https://docs.unsloth.ai)

* [Modelos - Hugging Face](https://huggingface.co/unsloth)

* [Discord oficial]()


<br><br>

## [Licen√ßa]()

<br>

Apache 2.0



<br><br>



## üíå [Let the data flow... Ping Me !](mailto:fabicampanari@proton.me)

<br>


#### <p align="center">  üõ∏‡πã My Contacts [Hub](https://linktr.ee/fabianacampanari)


<br>

### <p align="center"> <img src="https://github.com/user-attachments/assets/517fc573-7607-4c5d-82a7-38383cc0537d" />


<br><br>

<p align="center">  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚äπüî≠‡πã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

<!--
<p align="center">  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üõ∏‡πã*‡©à‚ú©* üî≠*‡©à‚Çä ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
-->

<br>

<p align="center"> ‚û£‚û¢‚û§ <a href="#top">Back to Top </a>
  

  
#
 


<br><br>

##### <p align="center"> Code released under the  [Apache Licencve.](https://github.com/Mindful-AI-Assistants/CDIA-Entrepreneurship-Soft-Skills-PUC-SP/blob/21961c2693169d461c6e05900e3d25e28a292297/LICENSE)

